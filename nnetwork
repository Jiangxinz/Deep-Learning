import numpy as np
import matplotlib.pyplot as plt
import h5py

#sigmoid函数
def sigmoid(z):
    return 1/(1+np.exp(-z))
    
#每一层的神经元数
def layer_size(i, o):
    n_x = i.shape[0]
    n_y = o.shape[0]
    
    return n_x, n_y
    
#初始化参数
def init_param(nx, nh, ny):
    np.random.seed(2)
    
    W1 = np.random.randn(nh, nx) * 0.01
    b1 = np.zeros((nh, 1))
    W2 = np.random.randn(ny, nh) * 0.01
    b2 = np.zeros((1,1))
    
    param = {"W1":W1,"b1":b1,"W2":W2,"b2":b2}
    
    return param
    
#前向传播
def forward_propagate(X, param):
    W1 = param["W1"]
    W2 = param["W2"]
    
    b1 = param["b1"]
    b2 = param["b2"]
    
    Z1 = np.dot(W1, X) + b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)
    
    p = {"Z1":Z1, "Z2":Z2, "A1":A1, "A2":A2}
    return p
    
#成本函数
def costs(A2, Y):
    n = Y.shape[1]
    
    cost = -np.sum(np.multiply(Y,np.log(A2)) + np.multiply((1-Y),np.log(1-A2)))/n
    #cost = np.squeeze(cost)
    return cost
    
#反向传播
def backward_propagate(X, Y, parameters, fp):
    n = Y.shape[1]
    
    #W1 = parameters["W1"]
    W2 = parameters["W2"]
    
    A1 = fp["A1"]
    A2 = fp["A2"]
    
    dZ2 = A2 - Y
    dW2 = np.dot(dZ2, A1.T) / n
    db2 = np.sum(dZ2, axis = 1, keepdims = True) / n
    
    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))
    dW1 = np.dot(dZ1, X.T) / n
    db1 = np.sum(dZ1, axis = 1, keepdims = True) / n
    
    grads = {"dW1": dW1, "dW2":dW2,"db1":db1,"db2":db2}
    return grads

#优化参数    
def optimize_param(param, grads, lr):
    W1 = param["W1"]
    W2 = param["W2"]
    b1 = param["b1"]
    b2 = param["b2"]
    
    dW1 = grads["dW1"]
    dW2 = grads["dW2"]
    db1 = grads["db1"]
    db2 = grads["db2"]
    
    W1 = W1 - lr * dW1
    W2 = W2 - lr * dW2
    
    b1 = b1 - lr * db1
    b2 = b2 - lr * db2
    
    param = {"W1":W1,"W2":W2,"b2":b2,"b1":b1}
    return param

#预测结果  
def predict(param, X):
    p = forward_propagate(X, param)
    pred = (p["A2"] > 0.5)
    
    return pred
    
#神经网络模型
def model(X, Y, nh, lr = 0.05, num_iterations = 5000, print_cost = False):
    ni, no = layer_size(X,Y)
    
    param = init_param(ni, nh, no)
    
    #W1 = param["W1"]
    #W2 = param["W2"]
    #b1 = param["b1"]
    #b2 = param["b2"]
    e = []
    for i in range(num_iterations):
        p = forward_propagate(X, param)
        cost = costs(p["A2"], Y)
        e.append(cost)
        grads = backward_propagate(X, Y, param, p)
        param = optimize_param(param, grads, lr)
        
        if print_cost and i % 100 == 0:
            print("迭代%i次后的成本: %f" %(i, cost))
    return param, e

def load_dataset():
    train_set = h5py.File("D:train_cat.h5","r")
    test_set = h5py.File("D:test_cat.h5","r")
    
    train_set_x = np.array(train_set["train_set_x"][:])
    train_set_y = np.array(train_set["train_set_y"][:])
    
    test_set_y = np.array(test_set["test_set_y"][:])
    test_set_x = np.array(test_set["test_set_x"][:])
    
    train_set_y = train_set_y.reshape(1,train_set_y.shape[0])
    test_set_y = test_set_y.reshape(1,test_set_y.shape[0])
    
    #classes = test_set["list_classes"][:]
    
    train_set_x = train_set_x.reshape(train_set_x.shape[0],-1).T
    test_set_x = test_set_x.reshape(test_set_x.shape[0],-1).T
    
    train_set_x = train_set_x / 255.0
    test_set_x = test_set_x /255.0
    
    return train_set_x, train_set_y, test_set_x, test_set_y
    
X, Y,x, y = load_dataset()

#隐层不同神经元数目的训练结果
list_costs = []   #不同神经元数目的成本

for h in [2,3,4,5,10,20]:
    param ,e= model(X, Y, nh = h, num_iterations = 4000, print_cost = True)
    predictions1 = predict(param, X)
    predictions2 = predict(param, x)
    list_costs.append(e)
    accuracy1 = float((np.dot(Y,predictions1.T) + np.dot(1-Y,1-predictions1.T))/float(Y.size)*100)
    accuracy2 = float((np.dot(y,predictions2.T) + np.dot(1-y,1-predictions2.T))/float(y.size)*100)
    print ("节点数为{}时的训练集识别准确度为 : {} %".format(h, accuracy1))
    print ("节点数为{}时的测试集识别准确度为 : {} %".format(h, accuracy2))
